{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f6ec13-d9e9-4828-8726-d8cbe00561bd",
   "metadata": {},
   "source": [
    "# Machine Learning Course Homework1 : Implementation of Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a680af0-e96d-46ca-bc6a-879103e6627c",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c27e8d-1ed0-465a-bc79-86e037e66bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import random\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a355a-f94e-4a2a-8a43-a11201d4fa17",
   "metadata": {},
   "source": [
    "## A decorator to measure running time of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c379af-015f-4ce9-ac46-7a775b96997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timer(func):\n",
    "    from time import time\n",
    "    def wrapper(*args,**kwargs):\n",
    "        t1 = time()\n",
    "        result= func(*args,**kwargs)\n",
    "        t2=time()\n",
    "        print(f\"Running time:{t2-t1}\")\n",
    "        #print(result)\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f169683-216c-41a3-a215-b6392d4df408",
   "metadata": {},
   "source": [
    "# a : Generating Data\n",
    "## inputs:\n",
    "\n",
    "n : number of samples\n",
    "\n",
    "p : number of predictors\n",
    "\n",
    "q : number of predictors related to the target variable\n",
    "\n",
    "## Output:\n",
    "\n",
    "y (n*1 vector) : linear combination of q columns of X\n",
    "\n",
    "X (n*p matrix) : n samples and p features\n",
    "\n",
    "Beta(p*1 vector): coefficients of the linear combination ( q of them are non-zero)\n",
    "\n",
    "\n",
    "epsilon : noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d364d127-5436-4c9c-bcba-05439b107082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to print elements of the array with 4 digits.\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "@timer\n",
    "\n",
    "def generate_data(n,p,q,data_mean=0,data_sd=1,\n",
    "                 parameter_mean=0,parameter_sd=1,\n",
    "                 noise_sd=0.05,verbose=False):\n",
    "    '''This function generates random data to test for Linear regression. \n",
    "    n: number of samples\n",
    "    p: number of features\n",
    "    q: number of features that have an influence on y.\n",
    "    data_mean: average of the randomly generated data\n",
    "    data_sd : standard deviation of the randomly generated data\n",
    "    parameter_mean: mean of Beta's. (coefficients of the linear combination of q features of X)\n",
    "    paramter_sd : standard deviation of Beta's\n",
    "    noise_sd: random noise added to y's'''\n",
    "    \n",
    "    X= random.normal(data_mean,data_sd,size=(n,p))\n",
    "    q_columns = sample(range(1,p),q)\n",
    "    #print(q_columns)\n",
    "    Beta=np.zeros((p,1))\n",
    "    Beta[q_columns,:]= random.normal(parameter_mean,parameter_sd,size=(q,1))\n",
    "    #print(Beta)\n",
    "    \n",
    "    y = np.matmul(X,Beta)\n",
    "    epsilon = random.normal(0,0.05,size=(n,1))\n",
    "    y_noisy = y+epsilon\n",
    "    if verbose:\n",
    "        print(\"X:\\n\", X )\n",
    "        print(\"Beta:\\n\",Beta)\n",
    "        print(\"y:\\n\",y)\n",
    "        print(\"noise(epsilon):\\n\",epsilon)\n",
    "        print(\"y_noisy:\\n\",y_noisy)\n",
    "    return (y_noisy,X,Beta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8a886a-f19f-4a58-a0b7-ba3c2d407b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p,q=10,5,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422cbe2a-6d2c-4c0c-9924-81951c14af79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[-0.6292 -1.0609  1.9288  0.4032 -0.2342]\n",
      " [ 0.9156  0.0342 -0.5148  0.631  -0.6107]\n",
      " [-0.2429 -1.6278 -1.5726  0.5255  0.6506]\n",
      " [ 1.1362  0.4375 -2.0785 -1.1043  0.1441]\n",
      " [ 0.2814  2.5442 -0.3074 -0.1756 -2.3842]\n",
      " [ 1.177  -0.1067 -0.3186  1.9911  0.3701]\n",
      " [ 0.0222 -0.6367 -0.1493  0.1599 -1.1548]\n",
      " [ 1.2016 -1.0395  0.9269  1.0811 -0.5587]\n",
      " [-0.9256 -0.0263  0.3468 -0.9185  1.1463]\n",
      " [-0.2452 -0.5455 -0.642   1.0839  0.8628]]\n",
      "Beta:\n",
      " [[ 0.    ]\n",
      " [-0.7973]\n",
      " [-0.0088]\n",
      " [ 0.    ]\n",
      " [ 0.3325]]\n",
      "y:\n",
      " [[ 0.751 ]\n",
      " [-0.2259]\n",
      " [ 1.5279]\n",
      " [-0.2826]\n",
      " [-2.8186]\n",
      " [ 0.2109]\n",
      " [ 0.1249]\n",
      " [ 0.6349]\n",
      " [ 0.3991]\n",
      " [ 0.7274]]\n",
      "noise(epsilon):\n",
      " [[-0.0191]\n",
      " [ 0.0292]\n",
      " [ 0.0608]\n",
      " [-0.0838]\n",
      " [-0.0348]\n",
      " [-0.0244]\n",
      " [ 0.0489]\n",
      " [ 0.0606]\n",
      " [-0.052 ]\n",
      " [-0.0268]]\n",
      "y_noisy:\n",
      " [[ 0.732 ]\n",
      " [-0.1966]\n",
      " [ 1.5888]\n",
      " [-0.3664]\n",
      " [-2.8534]\n",
      " [ 0.1865]\n",
      " [ 0.1738]\n",
      " [ 0.6955]\n",
      " [ 0.3471]\n",
      " [ 0.7006]]\n",
      "Running time:0.061463356018066406\n"
     ]
    }
   ],
   "source": [
    "y,X,Beta_real = generate_data(n,p,q,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92077444-1945-42f0-83a9-83ab5e0aa417",
   "metadata": {},
   "source": [
    "# b: Linear Regression using closed-form formula\n",
    "## inputs:\n",
    "\n",
    "X: n*p matrix \n",
    "\n",
    "y:n*1 vector\n",
    "\n",
    "## Output:\n",
    "\n",
    "Solve the linear regression :\n",
    "\n",
    "Beta: matrix p*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16d1bcf5-30dd-4c1e-8cbb-4d6ef32cd296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    ],\n",
       "       [ 1.5454],\n",
       "       [-0.2233],\n",
       "       [ 0.    ],\n",
       "       [-0.6751]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9ddbeab-0483-49c7-96a1-7b0d3010be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def linear_regression_formula(X,y):\n",
    "\n",
    "    Beta_estimated= np.linalg.inv(X.T@X)@X.T@y\n",
    "    \n",
    "    print(\"Beta_estimated by Linear Regression Closed Formula:\\n\",Beta_estimated)\n",
    "    return Beta_estimated\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cef3ec97-8c45-4778-9f17-f10f233197aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta_estimated by Linear Regression Closed Formula:\n",
      " [[ 0.0033]\n",
      " [ 1.5503]\n",
      " [-0.2233]\n",
      " [ 0.0084]\n",
      " [-0.6683]]\n",
      "Running time:0.13397789001464844\n"
     ]
    }
   ],
   "source": [
    "Beta_estimated_linear_regression=linear_regression_formula(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee98021-8b1e-42ab-b55e-7182fa33cfae",
   "metadata": {},
   "source": [
    "# c: Linear Regression using Gradient Descent\n",
    "## inputs:\n",
    "\n",
    "X: n*p matrix \n",
    "\n",
    "y:n*1 vector\n",
    "\n",
    "alpha: learning rate\n",
    "\n",
    "th: convergence threshold (percentage)\n",
    "\n",
    "max_iter: maximum number of iterations\n",
    "\n",
    "## Output:\n",
    "\n",
    "Solve the linear regression :\n",
    "\n",
    "Beta: matrix p*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5b9a541-0314-4ece-99d7-f3cba439bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def gradient_descent_batch(X,y,alpha,th,max_iter,verbose=False):\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    n,p=X.shape\n",
    "    Beta= random.normal(0,1,size=(p,1))\n",
    "    objective_function=np.inf\n",
    "    for epoch in range(max_iter):\n",
    "        diff = y-(X@Beta)\n",
    "        updated_objective_function= 0.5*(np.transpose(diff)@diff)\n",
    "        change=abs((updated_objective_function-objective_function)/objective_function)\n",
    "        if verbose:\n",
    "            print(f\"epoch:{epoch}\")\n",
    "            print(f\"change :{change}\")\n",
    "            print()\n",
    "        if change < th:\n",
    "            \n",
    "                    \n",
    "            print(f'Threshold Reached at epoch:{epoch}')\n",
    "            break\n",
    "            \n",
    "        #change=(updated_objective_function-objective_function)/objective_function\n",
    "        #print(f\"change:{change}\")\n",
    "        \n",
    "        objective_function= updated_objective_function\n",
    "            \n",
    "        grad_objective_function=  -1*(np.transpose(X)@y)+(np.transpose(X)@X@Beta)\n",
    "        Beta= Beta - alpha*grad_objective_function\n",
    "    print(\"Beta_estimated by Gradient Descent:\\n\",Beta)\n",
    "        \n",
    "    return Beta\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c9ad8-6056-4f3f-9a4d-75f0bee35ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6058fbae-784d-4424-a30f-36991b79b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def gradient_descent_stochastic(X,y,alpha,th,max_iter,verbose=False):\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    n,p=X.shape\n",
    "    Beta= random.normal(0,1,size=(p,1))\n",
    "    objective_function=np.inf\n",
    "    for epoch in range(max_iter):\n",
    "        diff = y-(X@Beta)\n",
    "        updated_objective_function= 0.5*(np.transpose(diff)@diff)\n",
    "        change=abs((updated_objective_function-objective_function)/objective_function)\n",
    "        if verbose:\n",
    "            print(f\"epoch:{epoch}\")\n",
    "            print(f\"change :{change}\")\n",
    "            print()\n",
    "        if change < th:\n",
    "            \n",
    "                    \n",
    "            #print(f'Threshold Reached at epoch:{epoch}')\n",
    "            pass\n",
    "            \n",
    "        #change=(updated_objective_function-objective_function)/objective_function\n",
    "        #print(f\"change:{change}\")\n",
    "        \n",
    "        objective_function= updated_objective_function\n",
    "        i = sample(range(n),1)    \n",
    "        grad_objective_function=  -1*(X[i,:].T)*(y[i]-X[i,:]@Beta)\n",
    "        Beta= Beta - alpha*grad_objective_function\n",
    "    print(\"Beta_estimated by Gradient Descent:\\n\",Beta)\n",
    "        \n",
    "    return Beta\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1e6811e9-568c-4fa6-9294-83b87decebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta_estimated by Gradient Descent:\n",
      " [[-0.0095]\n",
      " [-0.8335]\n",
      " [-0.014 ]\n",
      " [ 0.0072]\n",
      " [ 0.3024]]\n",
      "Running time:0.29642200469970703\n"
     ]
    }
   ],
   "source": [
    "Beta_estimated_gradient_descent_stochastic=gradient_descent_stochastic(X,y,alpha=0.01,th=0.0005,max_iter=10000,verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "44be0382-5cff-460b-a0cf-1371a0dc9aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(range(n),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2d430e5-affc-457f-851a-7708a6105397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold Reached at epoch:165\n",
      "Beta_estimated by Gradient Descent:\n",
      " [[-0.0146]\n",
      " [-0.8335]\n",
      " [-0.0151]\n",
      " [ 0.0097]\n",
      " [ 0.2982]]\n",
      "Running time:0.0034296512603759766\n"
     ]
    }
   ],
   "source": [
    "Beta_estimated_gradient_descent=gradient_descent_batch(X,y,alpha=0.01,th=0.0005,max_iter=10000,verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "31068995-7531-4928-8ba1-fac7b1b933f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    ],\n",
       "       [ 1.5454],\n",
       "       [-0.2233],\n",
       "       [ 0.    ],\n",
       "       [-0.6751]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta_real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0856db-084f-4272-8219-d80198cf598b",
   "metadata": {},
   "source": [
    "# d: Comparing the results with the scikit learn implemenation\n",
    "## inputs:\n",
    "\n",
    "X: n*p matrix \n",
    "\n",
    "y:n*1 vector\n",
    "\n",
    "\n",
    "## Output:\n",
    "\n",
    "Solve the linear regression :\n",
    "\n",
    "Beta: matrix p*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "97708016-0750-42a5-9df2-bfe394a71c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "36ae51d5-4af9-47a9-b61f-16fd2db77dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0089],\n",
       "       [ 1.5266],\n",
       "       [-0.2299],\n",
       "       [ 0.0129],\n",
       "       [-0.6312]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.coef_.reshape(p,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4090f58b-653b-4586-af93-c92e6cd2fbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991152967464308"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7c3e3-df79-47c5-b80a-10eaa077154d",
   "metadata": {},
   "source": [
    "### Comparing The Results:\n",
    "Beta: A matrix with the following columns\n",
    "\n",
    "#### [Actual Beta,,, Beta estimated by Linear Regression closed formula,,,Beta estimated by gradient descent,,,Beta stimated by scikitlearn ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7dc777a9-6cd8-403a-a553-21ee05f97157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    ,  0.0033,  0.0035,  0.0089],\n",
       "       [ 1.5454,  1.5503,  1.5454,  1.5266],\n",
       "       [-0.2233, -0.2233, -0.2297, -0.2299],\n",
       "       [ 0.    ,  0.0084,  0.0133,  0.0129],\n",
       "       [-0.6751, -0.6683, -0.6647, -0.6312]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta_complete=np.hstack((Beta_real,Beta_estimated_linear_regression,Beta_estimated_gradient_descent,lm.coef_.reshape(p,1)))\n",
    "Beta_complete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "136a5a3b-2ae0-435b-a993-06e1acb69388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to compare all the results by giving the input parameters.\n",
    "def compare_results(n,p,q):\n",
    "    y,X,Beta_real = generate_data(n,p,q)\n",
    "    Beta_estimated_linear_regression=linear_regression_formula(X,y)\n",
    "    Beta_estimated_gradient_descent=gradient_descent_batch(X,y,alpha=0.01,th=0.00005,max_iter=10000) \n",
    "    lm= LinearRegression()\n",
    "    lm.fit(X,y)\n",
    "    Beta_estimated_scikitlearn = lm.coef_.reshape(p,-1)\n",
    "    Beta_complete=np.hstack((Beta_real,Beta_estimated_linear_regression,Beta_estimated_gradient_descent,lm.coef_.reshape(p,1)))\n",
    "    return Beta_complete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f5e2eaf4-3e36-4f2e-a18b-278ddc0f97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to compare the mean squared error of our models. \n",
    "def compare_mse(n,p,q):\n",
    "    y,X,Beta_real = generate_data(n,p,q)\n",
    "    Beta_estimated_linear_regression=linear_regression_formula(X,y)\n",
    "    Beta_estimated_gradient_descent=gradient_descent_batch(X,y,alpha=0.01,th=0.00005,max_iter=10000) \n",
    "    lm= LinearRegression()\n",
    "    lm.fit(X,y)\n",
    "    Beta_estimated_scikitlearn = lm.coef_.reshape(p,-1)\n",
    "    regression_mse= (1/n)*((Beta_estimated_linear_regression-Beta_real)**2).sum()\n",
    "    gradient_mse= (1/n)*((Beta_estimated_gradient_descent-Beta_real)**2).sum()\n",
    "    scikit_mse= (1/n)*((Beta_estimated_scikitlearn-Beta_real)**2).sum()\n",
    "    print()\n",
    "    print('MSE: Linear Regression Closed Formula, Gradient Descent, Scikit Learn')    \n",
    "    return np.array((regression_mse,gradient_mse,scikit_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "037699fc-4068-4dbd-9a5c-d0e38c20c520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time:0.0\n",
      "Beta_estimated by Linear Regression Closed Formula:\n",
      " [[-0.0549]\n",
      " [-0.0023]\n",
      " [-0.708 ]\n",
      " [ 0.0329]\n",
      " [ 1.0736]]\n",
      "Running time:0.0010075569152832031\n",
      "Beta_estimated by Gradient Descent:\n",
      " [[-0.0578]\n",
      " [-0.0024]\n",
      " [-0.7078]\n",
      " [ 0.0338]\n",
      " [ 1.0742]]\n",
      "Running time:0.007517814636230469\n",
      "\n",
      "MSE: Linear Regression Closed Formula, Gradient Descent, Scikit Learn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.0004, 0.0005, 0.0003])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_mse(10,5,2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a900e9a6-af77-46c2-9561-bc2acbfc66ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time:0.0\n",
      "Beta_estimated by Linear Regression Closed Formula:\n",
      " [[-0.0097]\n",
      " [ 0.0098]\n",
      " [ 0.7659]]\n",
      "Running time:0.0\n",
      "Beta_estimated by Gradient Descent:\n",
      " [[-0.0097]\n",
      " [ 0.0113]\n",
      " [ 0.7656]]\n",
      "Running time:0.004022359848022461\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = compare_results(n=10,p=3,q=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fe096-3c4c-4c17-84ce-0a221811e579",
   "metadata": {},
   "source": [
    "# e: Lasso Regression: Solve by Cyclic coordinate descent and compare with scikit learn\n",
    "## inputs:\n",
    "\n",
    "X: n*p matrix \n",
    "\n",
    "y:n*1 vector\n",
    "\n",
    "\n",
    "## Output:\n",
    "\n",
    "Solve the linear regression :\n",
    "\n",
    "Beta: matrix p*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "569836b8-aa2d-45e5-9303-26da83f981e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(x,lamb):\n",
    "    if x>lamb:\n",
    "        return (x-lamb)\n",
    "    elif x<-lamb:\n",
    "        return (x+lamb)\n",
    "    else:\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc5f050-c13e-400c-89f8-283bdef6a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def cyclic_coordinate_descent(X:'matrix X',y:'a vector',lamb:'penalty hyperparameter',\n",
    "                              th:'convergence threshold(percentage of change)',max_iter:'maximum number of iterations'=100,\n",
    "                              verbose:'to report change of parameters at each epoch '=False):\n",
    "    n,p=X.shape\n",
    "    lamb=lamb*n\n",
    "    # I had to multiply lambda by n(number of samples) so that the results would match the scikit learn function.\n",
    "    X= X/(np.linalg.norm(X,axis=0))\n",
    "    #y = y/(np.linalg.norm(y,axis=0))\n",
    "    #print(X)\n",
    "    Beta= np.ones(p).reshape(-1,1)\n",
    "    prev_Beta= np.inf * np.ones(p).reshape(-1,1)\n",
    "    #print(Beta)\n",
    "    #print(prev_Beta)\n",
    "    for epoch in range(max_iter):\n",
    "        for j in range(p):\n",
    "            X_j = X[:,j].reshape(-1,1)\n",
    "            y_pred = X@Beta \n",
    "            r=  (y - y_pred + Beta[j]*X_j)\n",
    "            \n",
    "            #print(r)\n",
    "            Beta[j]=soft_threshold((X_j.T)@r,lamb)\n",
    "        change= 100*np.linalg.norm(Beta- prev_Beta)\n",
    "        if verbose:\n",
    "            print(f\"change:{change}\")\n",
    "        prev_Beta= Beta.copy()\n",
    "        if change<th:\n",
    "            print(f\"Threshold Reached at epoch:{epoch}\")\n",
    "            break\n",
    "    print(\"Beta_estimated by lasso regression solved by Cyclic coordinate Descent:\\n\",Beta)\n",
    "    return Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a30e6cf1-7714-49cc-8408-5d5bde3f5fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold Reached at epoch:12\n",
      "Beta_estimated by lasso regression solved by Cyclic coordinate Descent:\n",
      " [[ 0.    ]\n",
      " [-2.8469]\n",
      " [ 0.    ]\n",
      " [ 0.    ]\n",
      " [ 0.9184]]\n",
      "Running time:0.0020127296447753906\n"
     ]
    }
   ],
   "source": [
    "Beta_estimated_lasso_cyclic_coordinate= cyclic_coordinate_descent(X,y,lamb=0.01,th=0.0001,max_iter=100,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4a1a6-6918-4a5a-a7c7-a8e31dffb0f9",
   "metadata": {},
   "source": [
    "### In Scikit learn:\n",
    "alpha == lamba\n",
    "\n",
    "tol == th \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29dd8ddb-9571-477c-8d77-5c5f52ef427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta_estimated by lasso regression solved by scikitlearn:\n",
      " [[-0.    ]\n",
      " [-2.847 ]\n",
      " [-0.    ]\n",
      " [ 0.    ]\n",
      " [ 0.9184]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.01,tol=0.0001,max_iter=100,fit_intercept=False)\n",
    "\n",
    "clf.fit(X/np.linalg.norm(X,axis=0),y)\n",
    "Beta_estimated_lasso_scikitlearn=clf.coef_.reshape(-1,1)\n",
    "print(\"Beta_estimated by lasso regression solved by scikitlearn:\\n\",Beta_estimated_lasso_scikitlearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cec099-5095-4886-873e-e5b8e74cdbfb",
   "metadata": {},
   "source": [
    "### Comparing The Results:\n",
    "Beta:\n",
    "\n",
    "#### [Actual Beta,,, Beta estimated by cyclic coordinate lasso,,,Beta estimated by scikit learn lasso ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02dad48c-f83c-4003-a504-44373504ee00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    ,  0.    , -0.    ],\n",
       "       [-0.7973, -2.8469, -2.847 ],\n",
       "       [-0.0088,  0.    , -0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ],\n",
       "       [ 0.3325,  0.9184,  0.9184]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta_complete=np.hstack((Beta_real,Beta_estimated_lasso_cyclic_coordinate,Beta_estimated_lasso_scikitlearn))\n",
    "Beta_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545d6a6-a0b8-4c19-b30e-da2d4f991ed8",
   "metadata": {},
   "source": [
    "The results are almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d29750-4c6a-4696-bd98-aae7e293173b",
   "metadata": {},
   "source": [
    "# Resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8434d-4d48-41fe-b4b9-4e3d37860c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6697b4a2-ebbb-45dc-99dc-036b20380e5b",
   "metadata": {},
   "source": [
    "## Google Search:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dcfa9f-8ac3-478c-8b49-92f6f0e59150",
   "metadata": {},
   "source": [
    "### 1- inverse of a matrix in python:\n",
    "\n",
    "https://www.geeksforgeeks.org/how-to-inverse-a-matrix-using-numpy/\n",
    "\n",
    "### 2- gradient descent matrix form\n",
    "\n",
    "https://frickp.github.io/matrix-gradient-descent.html\n",
    "\n",
    "### 3- convergence threshold of gradient descent\n",
    "\n",
    "https://stackoverflow.com/questions/17289082/gradient-descent-convergence-how-to-decide-convergence\n",
    "\n",
    "### 4- How to implement lasso regression in python\n",
    "\n",
    "https://xavierbourretsicotte.github.io/lasso_implementation.html\n",
    "\n",
    "### 5- np.linalg.norm\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "### 6- How to print a numpy matrix in a pretty format(with fewer digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db8146-a1c2-414d-b707-9c83dccf681b",
   "metadata": {},
   "source": [
    "## ChatGPT:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72258c9f-e524-4d00-93dd-30579368be5d",
   "metadata": {},
   "source": [
    "### Prompt 1\n",
    "In python: do this and explain what you did\n",
    "Write a function that takes as input a matrix 𝑋, a vector 𝒚, a penalty\n",
    "hyperparameter of 𝜆, a convergence threshold 𝑡ℎ, and a maximum number of\n",
    "iterations max_iter, and solves the LASSO linear regression using cyclic\n",
    "coordinate descent algorithm. For simplicity, you can normalize 𝑋 so that its\n",
    "columns have zero mean and unit length, and normalize 𝑦 to have zero mean and\n",
    "unit length. Compare your results to the results of scikit-learn implementation of\n",
    "LASSO linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc326d40-6994-447a-a3dc-bbee7e193f87",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "I'd be glad to provide a comprehensive response that addresses the issues in the original code, incorporates insights from the ratings, and aims for exceptional quality:\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def lasso_cyclic_coordinate_descent(X, y, lmbda, th, max_iter):\n",
    "    # Normalize X and y\n",
    "    X_normalized = normalize(X, axis=0)\n",
    "    y_normalized = (y - np.mean(y)) / np.std(y)\n",
    "\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    X_transpose = X_normalized.T\n",
    "    residual = y_normalized.copy()\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        max_change = 0\n",
    "\n",
    "        for j in range(d):\n",
    "            # Compute the j-th coordinate of the gradient\n",
    "            rho = X_transpose[j].dot(residual - w[j] * X_transpose[j])\n",
    "            \n",
    "            # Update the j-th coordinate of the weight vector\n",
    "            if j == 0:\n",
    "                w[j] = 0  # Do not penalize the intercept term\n",
    "            elif rho < -lmbda / 2:\n",
    "                w[j] = (rho + lmbda / 2) / np.sum(X_transpose[j] ** 2)\n",
    "            elif rho > lmbda / 2:\n",
    "                w[j] = (rho - lmbda / 2) / np.sum(X_transpose[j] ** 2)\n",
    "            else:\n",
    "                w[j] = 0\n",
    "\n",
    "            # Update the residual\n",
    "            change = w[j] * X_transpose[j]\n",
    "            residual -= change\n",
    "\n",
    "            # Keep track of the maximum change in weights\n",
    "            max_change = max(max_change, np.max(np.abs(change)))\n",
    "\n",
    "        # Check for convergence\n",
    "        if max_change < th:\n",
    "            break\n",
    "\n",
    "    return w\n",
    "\n",
    "# Generate some random data for testing\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5)\n",
    "y = 2 * X[:, 0] + 3 * X[:, 1] - 1.5 * X[:, 2] + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "# Set hyperparameters\n",
    "lmbda = 0.1\n",
    "th = 1e-5\n",
    "max_iter = 1000\n",
    "\n",
    "# Solve using the cyclic coordinate descent function\n",
    "lasso_weights = lasso_cyclic_coordinate_descent(X, y, lmbda, th, max_iter)\n",
    "\n",
    "# Compare with scikit-learn Lasso implementation\n",
    "lasso_sklearn = Lasso(alpha=lmbda, max_iter=max_iter, tol=th)\n",
    "lasso_sklearn.fit(X, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Weights from cyclic coordinate descent:\", lasso_weights)\n",
    "print(\"Weights from scikit-learn Lasso:\", lasso_sklearn.coef_)\n",
    "\n",
    "```\n",
    "print(\"Weights from scikit-learn Lasso:\", lasso_sklearn.coef_)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8617f469-0f71-4188-8eca-a0bcd39ddf1f",
   "metadata": {},
   "source": [
    "## Gemini:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b39936-9182-4c47-a777-d2a4878aa48b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prompt 1\n",
    "In python: do this and explain what you did\n",
    "Write a function that takes as input a matrix 𝑋, a vector 𝒚, a penalty\n",
    "hyperparameter of 𝜆, a convergence threshold 𝑡ℎ, and a maximum number of\n",
    "iterations max_iter, and solves the LASSO linear regression using cyclic\n",
    "coordinate descent algorithm. For simplicity, you can normalize 𝑋 so that its\n",
    "columns have zero mean and unit length, and normalize 𝑦 to have zero mean and\n",
    "unit length. Compare your results to the results of scikit-learn implementation of\n",
    "LASSO linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c9563-a503-4254-8f60-aff6107b0f73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Answer 1\n",
    "I'd be glad to provide a comprehensive response that addresses the issues in the original code, incorporates insights from the ratings, and aims for exceptional quality:\n",
    "\n",
    "Python\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def cyclic_coordinate_descent(X: np.ndarray, y: np.ndarray, lam: float, tol: float, max_iter: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solves the LASSO linear regression problem using cyclic coordinate descent.\n",
    "\n",
    "    Args:\n",
    "        X: A 2D NumPy array of shape (n_samples, n_features) representing the design matrix.\n",
    "        y: A 1D NumPy array of shape (n_samples,) representing the target vector.\n",
    "        lam: The LASSO penalty hyperparameter.\n",
    "        tol: The convergence tolerance.\n",
    "        max_iter: The maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        A 1D NumPy array of shape (n_features,) representing the estimated regression coefficients.\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "\n",
    "    # Normalize X to ensure features have zero mean and unit variance\n",
    "    X_normalized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    # Initialize the coefficient vector Beta\n",
    "    Beta = np.zeros(p)\n",
    "\n",
    "    # Initialize previous coefficients for convergence check\n",
    "    prev_Beta = np.inf * np.ones(p)\n",
    "\n",
    "    # Track change for convergence criterion\n",
    "    change = np.inf\n",
    "\n",
    "    for epoch in range(max_iter):\n",
    "        for j in range(p):\n",
    "            # Extract the jth column of X and reshape for vector multiplication\n",
    "            X_j = X_normalized[:, j].reshape(-1, 1)\n",
    "\n",
    "            # Calculate the prediction using current coefficients and normalized data\n",
    "            y_pred = X_normalized @ Beta\n",
    "\n",
    "            # Calculate the residual for the jth feature\n",
    "            r = y - y_pred + Beta[j] * X_j\n",
    "\n",
    "            # Update the jth coefficient using soft-thresholding with L1 penalty\n",
    "            Beta[j] = soft_thresholding(np.dot(X_j.T, r), lam)\n",
    "\n",
    "        # Check for convergence based on change in coefficients\n",
    "        change = np.linalg.norm(Beta - prev_Beta)\n",
    "        prev_Beta = Beta.copy()\n",
    "        if change < tol:\n",
    "            break\n",
    "\n",
    "    return Beta\n",
    "\n",
    "\n",
    "def soft_thresholding(z: np.ndarray, lam: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies soft-thresholding to a vector.\n",
    "\n",
    "    Args:\n",
    "        z: A NumPy array.\n",
    "        lam: The LASSO penalty hyperparameter.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array with the same shape as z, where elements smaller than\n",
    "        the threshold are set to zero.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sign(z) * np.maximum(np.abs(z) - lam, 0)\n",
    "```\n",
    "Use code with caution.\n",
    "Explanation of Improvements:\n",
    "\n",
    "Function Definitions: The code is organized into two well-defined functions: cyclic_coordinate_descent and soft_thresholding, improving readability and maintainability.\n",
    "Type Hints: Meaningful type hints are added using np.ndarray, enhancing code clarity and enabling type checking for static analysis or IDE integration.\n",
    "Comprehensive Docstrings: Both functions include docstrings that explain their purpose, arguments, and return values, making the code more user-friendly and understandable.\n",
    "Clear Variable Naming: The code uses consistent and descriptive variable names (e.g., X_normalized, y_pred, r), improving readability.\n",
    "Normalization: The code normalizes X to have zero mean and unit variance using (X - np.mean(X, axis=0)) / np.std(X, axis=0). This is crucial for ensuring the stability of the CCD algorithm when features have different scales.\n",
    "Convergence Check and Early Stopping: The code incorporates a convergence check using the norm of the difference between the current and previous estimated coefficients (prev_Beta). If the change falls below the tolerance tol, the loop terminates, preventing unnecessary iterations.\n",
    "Vectorized Operations: The code utilizes vectorized operations (e.g., matrix multiplication using @, element-wise operations) where possible to improve efficiency and readability.\n",
    "Consistent Formatting: The code adheres to consistent indentation and spacing, enhancing visual clarity.\n",
    "This revised code addresses the issues raised in the ratings and combines the best aspects of both Response A and Response B while providing additional improvements for clarity, efficiency, and maintainability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71ddcb-8056-4f59-a780-7cb0b060785b",
   "metadata": {},
   "source": [
    "## Books :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a7a708-414d-46c1-955a-45563006cd3b",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "1- Mathematics for Machine Learning\n",
    "\n",
    "\n",
    "2- Statistical Learning with Sparsity\n",
    "\n",
    "3- Elements of Statistical Learning\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2f74f-4b63-4d77-a3ef-edb9649ed55e",
   "metadata": {},
   "source": [
    "# Questions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0cacc4-f9fa-4920-bb21-8edc815916e9",
   "metadata": {},
   "source": [
    "<font size=\"3\">1- I had to multiply the lambda (penalty parameter) by n(number of samples) in the function I defined to match the results with scikit learn. I\n",
    "figured this by trial and error and don't know the reason behind it exactly</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990c38f-f2af-4211-b88c-f681197d33f3",
   "metadata": {},
   "source": [
    "<font size=\"3\">2- For large n, gradient descent doesn't work:</font>\n",
    "RuntimeWarning: overflow encountered in matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "97d818a3-d62c-4ddf-9721-2c73a143b0c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time:0.0\n",
      "Beta_estimated by Linear Regression Closed Formula:\n",
      " [[0.0009]\n",
      " [0.4095]\n",
      " [0.0022]]\n",
      "Running time:0.0009975433349609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSADEGH\\AppData\\Local\\Temp\\ipykernel_29596\\3066973625.py:10: RuntimeWarning: overflow encountered in matmul\n",
      "  updated_objective_function= 0.5*(np.transpose(diff)@diff)\n",
      "C:\\Users\\MSADEGH\\AppData\\Local\\Temp\\ipykernel_29596\\3066973625.py:27: RuntimeWarning: overflow encountered in matmul\n",
      "  grad_objective_function=  -1*(np.transpose(X)@y)+(np.transpose(X)@X@Beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta_estimated by Gradient Descent:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "Running time:0.3158760070800781\n"
     ]
    }
   ],
   "source": [
    "result = compare_results(n=1000,p=3,q=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fafb080-7c38-4e36-9125-b45fc9064da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
